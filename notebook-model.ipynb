{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run helpers_gru.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# name of model. Used for saving conventions\n",
    "name = 'recsys' # 'imusic'\n",
    "\n",
    "# set sise of data (number of samples). If None (suggested), full datasets are applied.\n",
    "limit = None\n",
    "\n",
    "# how often would you like to check results?\n",
    "show_every_n_batches = 3000\n",
    "\n",
    "# decide on wether to show full validation statistics. Computational time is high when this is True\n",
    "full_validation_stats = False\n",
    "\n",
    "# decide whether to log testing\n",
    "log_testing = True\n",
    "\n",
    "# top k products to determine accuracy\n",
    "top_k = 20\n",
    "\n",
    "notes = 'Final GRU Model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512 - Number of sequences running through the network in one pass.\n",
    "batch_size = 512\n",
    "\n",
    "# 50 - Embedding dimensions\n",
    "embed_dim = 300\n",
    "\n",
    "# The dropout drop probability when training on input. If you're network is overfitting, try decreasing this.\n",
    "x_drop_probability = 0.00\n",
    "\n",
    "# The dropout keep probability when training on RNN neurons. If you're network is overfitting, try decreasing this.\n",
    "rnn_keep_probability = 1.00\n",
    "\n",
    "# 100 - The number of units in the hidden layers.\n",
    "rnn_size = 200\n",
    "\n",
    "# 1\n",
    "num_layers = 1\n",
    "\n",
    "# Learning rate for training\n",
    "# typically 0.0001 up to 1: http://datascience.stackexchange.com/questions/410/choosing-a-learning-rate\n",
    "# best 2017 05 01: learning_rate = 0.0025\n",
    "learning_rate = 0.0025\n",
    "\n",
    "# 10 epochs\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model folder for hyperparameters, statistics and the model itself\n",
    "model_counter_path = 'models/model_counter.txt'\n",
    "if os.path.isfile(model_counter_path):\n",
    "    model_counter_file = open(model_counter_path, 'r')\n",
    "    model_count = int(model_counter_file.read())\n",
    "    model_counter_file.close()\n",
    "    model_counter_file = open(model_counter_path, 'w')\n",
    "    model_counter_file.write(str(model_count + 1))\n",
    "    model_counter_file.close()\n",
    "else:\n",
    "    model_counter_file = open(model_counter_path, 'w+')\n",
    "    model_count = 1000 # initial model count/number\n",
    "    model_counter_file.write(str(model_count + 1))\n",
    "    model_counter_file.close()\n",
    "\n",
    "model_path_dir = 'models/' + str(model_count) + '-' + name + '-' + time.strftime(\"%y%m%d\") + '/'\n",
    "if not os.path.exists(model_path_dir):\n",
    "    os.makedirs(model_path_dir)\n",
    "\n",
    "stats_file_path = model_path_dir + name + '-' + time.strftime(\"%y%m%d%H%M\") + '-statsfile' + '.txt'\n",
    "stats_file = open(stats_file_path, 'w+')\n",
    "stats_file.write('model number: {}\\n'.format(model_count))\n",
    "stats_file.write('name: {}\\n\\n'.format(name))\n",
    "stats_file.write('limit: {}\\n'.format(limit))\n",
    "stats_file.write('batch_size: {}\\n'.format(batch_size))\n",
    "stats_file.write('embed_dim: {}\\n'.format(embed_dim))\n",
    "stats_file.write('x_drop_probability: {}\\n'.format(x_drop_probability))\n",
    "stats_file.write('rnn_keep_probability: {}\\n'.format(rnn_keep_probability))\n",
    "stats_file.write('rnn_size: {}\\n'.format(rnn_size))\n",
    "stats_file.write('num_layers: {}\\n'.format(num_layers))\n",
    "stats_file.write('learning_rate: {}\\n'.format(learning_rate))\n",
    "stats_file.write('num_epochs: {}\\n'.format(num_epochs))\n",
    "stats_file.write('show_every_n_batches: {}\\n'.format(show_every_n_batches))\n",
    "stats_file.write('top_k: {}\\n'.format(top_k))\n",
    "stats_file.write('full_validation_stats: {}\\n'.format(full_validation_stats))\n",
    "stats_file.write('notes: {}\\n'.format(notes))\n",
    "stats_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/rsc15_train_tr.txt was successfully loaded in!\n",
      "data/rsc15_train_valid.txt was successfully loaded in!\n",
      "data/rsc15_test.txt was successfully loaded in!\n"
     ]
    }
   ],
   "source": [
    "if limit == None:\n",
    "    validation_limit = None\n",
    "    testing_limit = None\n",
    "else:\n",
    "    validation_limit = int(0.2 * limit)\n",
    "    testing_limit = int(0.2 * limit)\n",
    "\n",
    "tr_data = load_our_data(path='data/rsc15_train_tr.txt', limit=limit)\n",
    "va_data = load_our_data(path='data/rsc15_train_valid.txt', limit=validation_limit)\n",
    "te_data = load_our_data(path='data/rsc15_test.txt', limit=testing_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques in training   37483\n",
      "uniques in validation 6359\n",
      "uniques in testing    6751\n",
      "\n",
      "depth (unique items)  37483\n"
     ]
    }
   ],
   "source": [
    "# get number of unique products\n",
    "print('uniques in training  ', np.unique(tr_data['ItemId']).shape[0])\n",
    "print('uniques in validation', np.unique(va_data['ItemId']).shape[0])\n",
    "print('uniques in testing   ', np.unique(te_data['ItemId']).shape[0])\n",
    "uniques = np.unique(np.append(np.append(tr_data['ItemId'], va_data['ItemId']), te_data['ItemId']))\n",
    "depth = uniques.shape[0]\n",
    "print('\\ndepth (unique items) ', depth)\n",
    "if depth != np.unique(tr_data['ItemId']).shape[0]:\n",
    "    print('\\nWARNING! Number of uniques in training should equal the depth (uniques in full set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a lookup table\n",
    "items_to_int, int_to_items = create_lookup_tables(list(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 - Number of timesteps the rnn should take in\n",
    "timesteps = 19\n",
    "\n",
    "# Transforming and splitting the data\n",
    "X_tr, y_tr = transform_and_split_our_data(tr_data, timesteps)\n",
    "X_va, y_va = transform_and_split_our_data(va_data, timesteps)\n",
    "X_te, y_te = transform_and_split_our_data(te_data, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforming items to an integer ID\n",
    "X_tr, y_tr = transforming_item_to_int(X_tr, y_tr, items_to_int)\n",
    "X_va, y_va = transforming_item_to_int(X_va, y_va, items_to_int)\n",
    "X_te, y_te = transforming_item_to_int(X_te, y_te, items_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def variable_summaries(var):\n",
    "#     \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "#     with tf.name_scope('summaries'):\n",
    "#         mean = tf.reduce_mean(var)\n",
    "#         tf.summary.scalar('mean', mean)\n",
    "#         with tf.name_scope('stddev'):\n",
    "#             stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "#         tf.summary.scalar('stddev', stddev)\n",
    "#         tf.summary.scalar('max', tf.reduce_max(var))\n",
    "#         tf.summary.scalar('min', tf.reduce_min(var))\n",
    "#         tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset any existing graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# create new graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    items_size = len(int_to_items) + 1\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, timesteps], name='inputs')\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, timesteps], name='targets')\n",
    "\n",
    "    with tf.name_scope(\"other_placeholders\"):\n",
    "        lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        x_drop_prob = tf.placeholder(tf.float32, name='x_drop_prob')\n",
    "        rnn_keep_prob = tf.placeholder(tf.float32, name='rnn_keep_prob')\n",
    "    \n",
    "    with tf.name_scope(\"x_dropout\"):\n",
    "        inputs_dropped = tf.layers.dropout(inputs, rate=x_drop_prob)\n",
    "    \n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        embedding = tf.get_variable('embedding_matrix', [items_size, embed_dim])\n",
    "        rnn_inputs = tf.nn.embedding_lookup(embedding, inputs_dropped)\n",
    "\n",
    "    with tf.name_scope(\"cell\"):\n",
    "        cell = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "\n",
    "    with tf.name_scope(\"rnn_dropout\"):\n",
    "        cell_dropped = tf.contrib.rnn.DropoutWrapper(cell=cell,\n",
    "                                                     input_keep_prob=1,\n",
    "                                                     state_keep_prob=rnn_keep_prob,\n",
    "                                                     output_keep_prob=rnn_keep_prob,\n",
    "#                                                      variational_recurrent=True,\n",
    "                                                     input_size=rnn_size,\n",
    "                                                     dtype=tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"rnn_dropout\"):\n",
    "        rnn_layer = tf.contrib.rnn.MultiRNNCell([cell_dropped] * num_layers)\n",
    "\n",
    "    with tf.name_scope(\"initial_state\"):\n",
    "        initial_state = rnn_layer.zero_state(batch_size, tf.int32)\n",
    "        initial_state = tf.identity(initial_state, name='initial_state')\n",
    "\n",
    "    with tf.name_scope(\"rnn_output\"):\n",
    "        rnn_output, final_state = tf.nn.dynamic_rnn(rnn_layer, rnn_inputs, dtype=tf.float32)\n",
    "        final_state =  tf.identity(final_state, name='final_state')\n",
    "\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        logits = tf.contrib.layers.fully_connected(rnn_output,\n",
    "                                                   items_size,\n",
    "                                                   activation_fn=None,\n",
    "                                                   biases_initializer=tf.constant_initializer(0.1))\n",
    "    \n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        # y is our prediction\n",
    "        probs = tf.nn.softmax(logits, name='probs')\n",
    "        probs = tf.slice(probs, [0, 0, 1], [-1, -1, -1])\n",
    "        zeros = tf.zeros([batch_size, timesteps, 1], tf.float32)\n",
    "        probs = tf.concat([zeros, probs], 2)\n",
    "    \n",
    "    with tf.name_scope(\"masking\"):\n",
    "        # top k predictions: Shape = (batch_size, timesteps, k)\n",
    "        top_preds_values, top_preds = tf.nn.top_k(probs, k=top_k)\n",
    "\n",
    "        # making targets a 3D matrix and finding the mask values\n",
    "        targets_ = tf.tile(tf.expand_dims(targets, 2), [1, 1, top_k])\n",
    "        mask_3d = tf.sign(tf.to_float(targets_))\n",
    "        mask_2d = tf.sign(tf.to_float(targets))\n",
    "\n",
    "        equal_pad = tf.equal(tf.sign(tf.to_float(targets)), 0)\n",
    "        pad_ints = tf.cast(equal_pad, tf.int32)\n",
    "        pad_count = tf.reduce_sum(pad_ints)\n",
    "\n",
    "        multiplier = tf.to_float(tf.divide((tf.multiply(batch_size, timesteps)), (tf.multiply(batch_size, timesteps) - pad_count)))\n",
    "\n",
    "    with tf.name_scope(\"accuracy_calc\"):\n",
    "        # calculating accuracy with mask\n",
    "        correct_pred = tf.equal(top_preds, targets_)\n",
    "        cor_pred = tf.sign(tf.to_float(correct_pred))\n",
    "        mask_acc = tf.multiply(mask_3d, cor_pred)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.multiply(tf.reduce_mean(tf.cast(mask_acc, tf.float32)), top_k)\n",
    "        accuracy_ = tf.multiply(accuracy, multiplier)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=targets)\n",
    "        masked_losses = tf.multiply(mask_2d, loss)\n",
    "        cost = tf.reduce_mean(masked_losses)\n",
    "\n",
    "    with tf.name_scope('optimizer'):\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "    \n",
    "    with tf.name_scope(\"saver\"):\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        tf.summary.scalar(\"loss\", cost)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy_)\n",
    "        merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH       BATCH       TR LOSS  |  TR ACC  |  VA LOSS  |  VA ACC         TRAINING STATUS\n",
      " 1/50       0/15677       1.686  |  19.94%  |    2.067  |   0.05%          0 m   -: - (0.00%)\n",
      " 1/50    3000/15677       0.089  |  68.49%  |    0.423  |  15.49%         13 m  58:33 (0.38%)\n",
      " 1/50    6000/15677       0.101  |  69.86%  |    0.375  |  11.74%         26 m  58:13 (0.77%)\n",
      " 1/50    9000/15677       0.081  |  67.96%  |    0.361  |  17.16%         40 m  57:56 (1.15%)\n",
      " 1/50   12000/15677       0.087  |  72.80%  |    0.326  |  18.98%         53 m  57:42 (1.53%)\n",
      " 1/50   15000/15677       0.115  |  69.14%  |    0.268  |  45.46%         67 m  57:29 (1.91%)\n",
      "\n",
      "Epoch  1     Loss  |   Accuracy\n",
      "Training    0.360  |     61.36% (show_every_n_batches)\n",
      "Validation  0.146  |     71.42% (all validation batches)\n",
      "Testing     0.146  |     71.42% (all test batches)\n",
      "Model saved (models/1111-recsys-170522/recsys-1705221537.ckpt-1)\n",
      "\n",
      " 2/50    2323/15677       0.081  |  75.58%  |    0.246  |  45.71%         81 m  57:57 (2.30%)\n",
      " 2/50    5323/15677       0.088  |  73.65%  |    0.243  |  34.23%         95 m  57:38 (2.68%)\n",
      " 2/50    8323/15677       0.081  |  68.24%  |    0.198  |  28.94%        108 m  57:20 (3.06%)\n",
      " 2/50   11323/15677       0.099  |  70.89%  |    0.249  |  22.18%        122 m  57: 3 (3.44%)\n"
     ]
    }
   ],
   "source": [
    "# generate all batches for training, validation and testing data\n",
    "tr_batches = get_batches(X_tr, y_tr, batch_size)\n",
    "va_batches = get_batches(X_va, y_va, batch_size)\n",
    "te_batches = get_batches(X_te, y_te, batch_size)\n",
    "t0 = time.time() # initialize start of training\n",
    "etr_h = '-' # initialize estimated time remaining\n",
    "etr_m = '-'\n",
    "log_count = 0 # counter for show_every_n_batches\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    training_writer = tf.summary.FileWriter(model_path_dir + '/training', graph=graph)\n",
    "    validation_writer = tf.summary.FileWriter(model_path_dir + '/validation')\n",
    "    testing_writer = tf.summary.FileWriter(model_path_dir + '/testing')\n",
    "    \n",
    "    # print headers of runtime statistics\n",
    "    print('EPOCH       BATCH       TR LOSS  |  TR ACC  |  VA LOSS  |  VA ACC         TRAINING STATUS')\n",
    "    stats_file = open(stats_file_path, 'a')\n",
    "    stats_file.write('\\nEPOCH       BATCH       TR LOSS  |  TR ACC  |  VA LOSS  |  VA ACC         TRAINING STATUS\\n')\n",
    "    stats_file.close()\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        # generate initial state\n",
    "        state = sess.run(initial_state, {inputs: tr_batches[0][0]})\n",
    "\n",
    "        # create empty arrays to hold accuracies and losses for training batches\n",
    "        tr_epoch_acc, tr_epoch_loss, va_loss_array, va_acc_array = [], [], [], []\n",
    "        \n",
    "        # train model using the train_op optimizer\n",
    "        for tr_batch_i, (X_tr_input, y_tr_input) in enumerate(tr_batches):\n",
    "            feed = {\n",
    "                inputs: X_tr_input,\n",
    "                targets: y_tr_input,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate,\n",
    "                x_drop_prob: x_drop_probability,\n",
    "                rnn_keep_prob: rnn_keep_probability\n",
    "            }\n",
    "            state, _ = sess.run([final_state, train_op], feed_dict=feed)\n",
    "\n",
    "            # show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(tr_batches) + tr_batch_i) % show_every_n_batches == 0:\n",
    "                \n",
    "                log_count += 1\n",
    "                \n",
    "                training_summary, tr_loss, tr_acc = sess.run([merged, cost, accuracy_], feed_dict={inputs: X_tr_input,\n",
    "                                                                                                          targets: y_tr_input,\n",
    "                                                                                                          x_drop_prob: 0,\n",
    "                                                                                                          rnn_keep_prob: 1})\n",
    "                tr_epoch_loss.append(tr_loss)\n",
    "                tr_epoch_acc.append(tr_acc)\n",
    "                \n",
    "                # full validation check\n",
    "                if full_validation_stats:\n",
    "#                     va_loss_array, va_acc_array = [], []\n",
    "                    for va_batch_i, (X_va_input, y_va_input) in enumerate(va_batches):\n",
    "                        validation_summary, va_loss, va_acc = sess.run([merged, cost, accuracy_], feed_dict={inputs: X_va_input,\n",
    "                                                                                                             targets: y_va_input,\n",
    "                                                                                                             x_drop_prob: 0,\n",
    "                                                                                                             rnn_keep_prob: 1})\n",
    "                        va_loss_array.append(va_loss)\n",
    "                        va_acc_array.append(va_acc)\n",
    "                    va_loss = sum(va_loss_array) / len(va_loss_array)\n",
    "                    va_acc = sum(va_acc_array) / len(va_acc_array)\n",
    "                else:\n",
    "                    va_batch_i = random.randint(0, len(va_batches) - 1)\n",
    "                    validation_summary, va_loss, va_acc = sess.run([merged, cost, accuracy_], feed_dict={inputs: va_batches[va_batch_i][0],\n",
    "                                                                                                         targets: va_batches[va_batch_i][1],\n",
    "                                                                                                         x_drop_prob: 0,\n",
    "                                                                                                         rnn_keep_prob: 1})\n",
    "#                     va_loss_array.append(va_loss)\n",
    "#                     va_acc_array.append(va_acc)\n",
    "                \n",
    "                # useful statistics\n",
    "                progress = tr_batch_i + (len(tr_batches) * epoch_i) # current batch\n",
    "                progress_total = len(tr_batches) * num_epochs # total batches\n",
    "                progress_pct = progress / progress_total * 100\n",
    "                seconds_spent = (time.time() - t0)\n",
    "                if progress_pct > 0: # avoid devision by zero\n",
    "                    etr = int((seconds_spent * (100 / progress_pct - 1)) / 60) # sets estimated time remaining\n",
    "                    etr_h = int(etr / 60)\n",
    "                    etr_m = int(etr % 60)\n",
    "                \n",
    "                # print all statistics\n",
    "                print('{:>2}/{:>2}   {:>5}/{}       {:.3f}  |  {:>6.2%}  |    {:.3f}  |  {:>6.2%}       {:>4} m  {:>2}:{:>2} ({:>5.2%})'.format(\n",
    "                    epoch_i + 1, # for human\n",
    "                    num_epochs,\n",
    "                    tr_batch_i,\n",
    "                    len(tr_batches),\n",
    "                    tr_loss,\n",
    "                    tr_acc,\n",
    "                    va_loss,\n",
    "                    va_acc,\n",
    "                    int(seconds_spent / 60),\n",
    "                    etr_h,\n",
    "                    etr_m,\n",
    "                    progress_pct / 100\n",
    "                ))\n",
    "                \n",
    "                # print all statistics to stats_file\n",
    "                stats_file = open(stats_file_path, 'a')\n",
    "                stats_file.write('{:>2}/{:>2}   {:>5}/{}       {:.3f}  |  {:>6.2%}  |    {:.3f}  |  {:>6.2%}       {:>4} m  {:>2}:{:>2} ({:>5.2%})\\n'.format(\n",
    "                    epoch_i + 1, # for human\n",
    "                    num_epochs,\n",
    "                    tr_batch_i,\n",
    "                    len(tr_batches),\n",
    "                    tr_loss,\n",
    "                    tr_acc,\n",
    "                    va_loss,\n",
    "                    va_acc,\n",
    "                    int(seconds_spent / 60),\n",
    "                    etr_h,\n",
    "                    etr_m,\n",
    "                    progress_pct / 100\n",
    "                ))\n",
    "                stats_file.close()\n",
    "                training_writer.add_summary(training_summary, log_count)\n",
    "#                 validation_writer.add_summary(validation_summary, log_count)\n",
    "        \n",
    "        # add training stats after epoch\n",
    "#         training_writer.add_summary(training_summary, log_count)\n",
    "    \n",
    "        # full validation after each epoch\n",
    "        validation_summary = None\n",
    "        va_loss_array, va_acc_array = [], []\n",
    "        for va_batch_i, (X_va_input, y_va_input) in enumerate(va_batches):\n",
    "            validation_summary, va_loss, va_acc = sess.run([merged, cost, accuracy_], feed_dict={inputs: X_va_input, targets: y_va_input, x_drop_prob: 0, rnn_keep_prob: 1})\n",
    "            va_loss_array.append(va_loss)\n",
    "            va_acc_array.append(va_acc)\n",
    "        if(log_testing):\n",
    "            validation_writer.add_summary(validation_summary, log_count)\n",
    "        \n",
    "        # full test after each epoch\n",
    "        testing_summary = None\n",
    "        te_loss_array, te_acc_array = [], []\n",
    "        for te_batch_i, (X_te_input, y_te_input) in enumerate(te_batches):\n",
    "            testing_summary, te_loss, te_acc = sess.run([merged, cost, accuracy_], feed_dict={inputs: X_te_input, targets: y_te_input, x_drop_prob: 0, rnn_keep_prob: 1})\n",
    "            te_loss_array.append(te_loss)\n",
    "            te_acc_array.append(te_acc)\n",
    "        if(log_testing):\n",
    "            testing_writer.add_summary(testing_summary, log_count)\n",
    "\n",
    "        # print final epoch statistics\n",
    "        stats_file = open(stats_file_path, 'a')\n",
    "        print('\\nEpoch {:>2}     Loss  |   Accuracy'.format(epoch_i + 1))\n",
    "        stats_file.write('\\nEpoch {:>2}     Loss  |   Accuracy\\n'.format(epoch_i + 1))\n",
    "        if len(tr_batches) < show_every_n_batches: # check if no intermediate loss/acc stats was shown\n",
    "            print('No training statistics as batches ({}) < show_every_n_batches ({})'.format(len(tr_batches), show_every_n_batches))\n",
    "            stats_file.write('No training statistics as batches ({}) < show_every_n_batches ({})\\n'.format(len(tr_batches), show_every_n_batches))\n",
    "        else:\n",
    "            print('Training    {:.3f}  |     {:>6.2%} (show_every_n_batches)'.format(sum(tr_epoch_loss) / len(tr_epoch_loss), sum(tr_epoch_acc) / len(tr_epoch_acc)))\n",
    "            stats_file.write('Training    {:.3f}  |     {:>6.2%} (show_every_n_batches)\\n'.format(sum(tr_epoch_loss) / len(tr_epoch_loss), sum(tr_epoch_acc) / len(tr_epoch_acc)))\n",
    "        print('Validation  {:.3f}  |     {:>6.2%} (all validation batches)'.format(sum(va_loss_array) / len(va_loss_array), sum(va_acc_array) / len(va_acc_array)))\n",
    "        stats_file.write('Validation  {:.3f}  |     {:>6.2%} (all validation batches)\\n'.format(sum(va_loss_array) / len(va_loss_array), sum(va_acc_array) / len(va_acc_array)))\n",
    "        print('Testing     {:.3f}  |     {:>6.2%} (all test batches)'.format(sum(te_loss_array) / len(te_loss_array), sum(te_acc_array) / len(te_acc_array)))\n",
    "        stats_file.write('Testing     {:.3f}  |     {:>6.2%} (all test batches)\\n\\n'.format(sum(te_loss_array) / len(te_loss_array), sum(te_acc_array) / len(te_acc_array)))\n",
    "        stats_file.close()\n",
    "        \n",
    "        # Save Model\n",
    "        location = model_path_dir + name + '-' + time.strftime(\"%y%m%d%H%M\") + '.ckpt'\n",
    "        saved_location = saver.save(sess, location, global_step=epoch_i + 1)\n",
    "        print('Model saved ({})\\n'.format(saved_location))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
